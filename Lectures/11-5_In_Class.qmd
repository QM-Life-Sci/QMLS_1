---
title: "Unit 11: In Class Discussion"
author:
  - Elizabeth King
  - Kevin Middleton
format:
  revealjs:
    theme: [default, custom.scss]
    standalone: true
    self-contained: true
    logo: QMLS_Logo.png
    slide-number: true
    show-slide-number: all
code-annotations: hover
bibliography: QMLS_Bibliography.bib
csl: evolution.csl
---

```{r}
#| label: setup
#| message: false
#| warning: false

library(tidyverse)
library(cowplot)

theme_set(theme_cowplot())
```


## Course Evaluations

- Available now until May 10th
- Course
- Faculty
    - King
    - Middleton
- [https://evaluation.missouri.edu/](https://evaluation.missouri.edu/)


## Pseudoreplication

- I understand that experiments should be designed to attempt to avoid pseudoreplication as much as possible, but there are definitely going to be some experiments, such as those working with specimens from the fossil record, where pseudoreplication is unlikely if not impossible to be avoided due to the inability to assume complete independence of the analyses conducted. Therefore, if pseudoreplication seems to be almost inevitable for analyses based on these sorts of data, would we then be forced to account for the impacts of pseudoreplication in these studies, or are there other methods of experimental design we could use to try to reduce those impacts?


## Pseudoreplication

- I'm interested to know how we can distinguish between pseudoreplication and true replication in behavioral and ecological research. I know it might depend on the question that we are willing to answer, but in those kind of studies where there are no experimental settings I find it a bit difficult to distinguish.
- How are pseudoreplication, random, and nested terms related and/or different? Does adding the random and nested terms to the model help avoid pseudoreplication?
- There are experiments that are difficult to have a replicate usually by limited sample size or resources. How can we take advantage of the information gathered considering that there are no replicates? Would it make the study less robust?


## Repeatability

- Is it correct to interpret ICC as power?


## Power

- Effect size in power test is what is the minimum difference that I would like to detect?
- Could we go over what all factors will affect statistical power?
- Can you run power analyses for mixed models? 


## Power

- I have a question about how to figure out what our sample sizes need to be for our experiments, but I'm not sure I'm thinking about it correctly. When determining what our sample size should be for a given experiment, how do we know what difference in the data we want to be able to detect? Can we think "I want to detect a significant difference", so I need to have a certain amount of power, so I need a certain sample size?


## Power {.smaller} 

- I'm really confused on how to estimate certain variables when determining power/sample size/ alpha. For example, if I want to find the sample size I need for 0.8 power and alpha of 0.05, how can I estimate effect size? Specifically if I'm looking at many different outcome variables and not just using one model and statistical test, AND if I have no idea what the actual outcome will be compared to what is expected (example, I know what is normal biologically, and I hypothesize that the change in outcome will be lower than normal, but how do I pick a d that accurately reflects the amount of effect GIVEN my idea is just a hypothesis and I have no distinct idea of the amount of variation?)


## Power

- I understand we need to calculate the power in order to estimate the chances of false positives and false negatives and estimate the best sample size, but when and why do we need to look at the distribution of p values, particularly the sequential Benferroni distribution?


## Best practices

- I have a broad question. Are there any "good practices" or "standards" for reporting power and repeatability? These seem like important steps during the formulation and analysis of an experiment. Still, outside of the Type I and Type II errors and power, I have not heard of these topics and have never seen them reported in academic journals/studies. 


## Multiple Comparisons {.smaller} 

- Do we have to correct and adjust p values for any type of analyses? Or is it focused on datasets such as DNA, SNPs analyses where there are multiple tests run at once?
- Can you perform MCP (like FDR) on p values from different types of tests? For example, I'm confused if I perform this on the p values I get from every ANOVA, t-test, etc. from my entire data set when using different outcome variables. Like is this done on everything at the end or within explorations into one answer?
- What is the difference between doing Bonferroni as a correction before post hoc tests and doing it as a MCP?
- I also am really confused on if you're supposed to run multiple models for one hypothesis (like additive, random predictors, etc) and then just use this on the results from those models? 


## Multiple Comparisons

- I thought I understood false discovery and false positive rate, but my quiz answers say otherwise. Could we discuss the difference?
- Quiz 11-4


## A menu of MCPs {.smaller}

1. <s>Do nothing</s>
    - Not an option 
2. Methods to control the Family-Wise Error Rate (FWER):
    - MCs within a single linear model (e.g. Tukey, etc.; see 08-2)
    - Bonferroni correction
      - Not recommended - overly conservative
    - Sequential Bonferroni procedure
    - Randomization procedures to empirically control FWER 
6. Methods to control the False Discovery Rate (FDR)
    - False Discovery Rate Methods
    - _Positive_ False Discovery Rate Methods
    

## False discovery rate

Proposed by Benjamini and Hochberg [-@Benjamini1995-cw].

- Also see Curran-Everett [-@Curran-Everett2000-qv].

Controls FDR (i.e., rate of Type I errors), rather than FWER

$$\mbox{FDR} = \frac{\mbox{n False Positives}}{\mbox{n All Positives}}$$

e.g., I'm OK with 5% false positives *among the tests I judge as significant*.

Note: False Positive Rate = $\frac{\mbox{n False Positives}}{\mbox{n All Tests}}$

## FDR & FPR

You do 100 correlations. You find 22 are statistically significant at an $\alpha$ = 0.05. Let's say 5 are false positives.

1. What is the FPR?
2. What is the FDR?

$$\mbox{FPR} = \frac{\mbox{n False Positives}}{\mbox{n All Tests}}$$
$$\mbox{FDR} = \frac{\mbox{n False Positives}}{\mbox{n All Positives}}$$


## FDR & FPR

$$FPR = \frac{5}{100} = 0.05$$

$$FDR = \frac{5}{22} = 0.227$$

## What next? {.smaller}

When going over things in lecture, I feel like I understand and can apply it to the given problems. However, I'm having a lot of issues trying to extrapolate the information from this class to my own data analysis, specifically for more complex data with extra variables and possible nested predictors. I feel like I generally understand things conceptually, but I can't describe it in my own words or apply it to complex problems outside of the course which makes me think I don't actually understand what I'm doing. Do you have any recommendations for better conceptualization and understanding of the math / actions behind what we are doing if we need more examples and explanations than from lecture videos, rather than just resources explaining how to do it? (The lectures, which generally come from a place of teaching what we are doing, not just how to do it, are great; but I feel like I need to hear it again or in a different way for some topics. I'm having a hard time finding resources that explain the concepts and not just how to do them)


## References
