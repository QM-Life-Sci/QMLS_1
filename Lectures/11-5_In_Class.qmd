---
title: "Unit 11: In Class Discussion"
author:
  - Elizabeth King
  - Kevin Middleton
format:
  revealjs:
    theme: [default, custom.scss]
    standalone: true
    self-contained: true
    logo: QMLS_Logo.png
    slide-number: true
    show-slide-number: all
code-annotations: hover
bibliography: QMLS_Bibliography.bib
csl: evolution.csl
---

```{r}
#| label: setup
#| message: false
#| warning: false

library(tidyverse)
library(cowplot)

theme_set(theme_cowplot())
```

> From the last lecture in week 11, how do we know if our research has multiplicity of tests that we need to account for? 

## Pseudoreplication

- I am struggling to wrap my head around the idea of replication and pseudoreplication.

- I'm curious, is pseudoreplication always something to be avoided, or is it more of a neutral phenomenon that becomes a problem when you overstate what you can conclude from your setup?

- What sort of ways might it be useful to report data with limited replication or small sample size without overstating your conclusions?

- Quiz 11-1

## ICC 

- How does the ICC provide info on the degree of similarity among observations within the same group, and what considerations should we keep in mind when interpreting ICC values in studies?

## Decision Errors: General

- Given that we learned about errors and false positive rates, is there any way to ensure that any conclusions we made aren't due to an error? Let's say, we have small size due to measuring vertebrates. Like Kevin mentioned, it is harder to achieve some of these numbers needed for adequate power for some model organisms.

- How do we balance the need to control the familywise error rate to maintain the overall Type I error rate while avoiding overly conservative adjustments that may compromise statistical power?

## Multiple Comparisons {.smaller} 

- I am not understanding when/how to choose whether or not to use a regular Tukey test or Planned set of comparisons. You mentioned in the lectures that doing Planned set of comparisons can sometimes lead to "p-hacking", so I am still not sure when it is or isn't okay to pick and choose a specific set of comparisons. 

- In genomics studies where researchers typically perform numerous statistical tests simultaneously, such as testing the association between genetic variants (e.g., single nucleotide polymorphisms, or SNPs) and a trait of interest. With thousands or millions of tests, the likelihood of observing false positives (Type I errors) increases, would you suggest the use of  pFDR, FDR or FWER in this situation?

- I'm curious about some of the points that were hinted at on the last slide of 11-4. In a case where, for example, we're performing differential expression and have a dataset of 4 groups. We want to perform a differential expression for each pairwise combination of groups (1-2,1-3,1-4, ..,, etc). It is typical to correct for multiple hypotheses within each comparison (i.e. for each gene tested in 1-2 comparison). Should the p values be corrected for all hypotheses tested in the experiment as a whole?  What should be considered when making this decision?

## Multiple Comparisons

- In the last lecture, you talked about having ranges of pvalues. Where do those values come from? Do you combine the p values from different kinds of tests?

- In reference to lecture 4 about decision making and using multiple comparison procedures. At what point in your data do you decide it is necessary to run this? When in real life will we come across having to determine when to use these procedures?

- I understand that we should keep from doing nothing when doing multiple tests, but then why is it that some sources recommend doing nothing as an approach?

## Multiple Comparisons {.smaller}

- I'm having trouble understanding the difference between False Discovery Rate and False Positive Rate. In lecture 11-4, slide 17, the two equations are exactly the same. 

- How pFDR differs from FDR? Considering FWER, FDR, and pFDR, please clarify which is best for multiple comparisons. 

- I'm still a bit confused on pFDR and q-values. Specifically when we should use pDFR instead of FDR and vice versa. 

- I'm also confused about q values.  What situations would knowing the q value be more useful over p values?

- How FWER, FDR and pFDR differ from each other?

- Quiz 11-4 Q1

## A menu of MCPs {.smaller}

1. <s>Do nothing</s>
    - Not an option 
2. Methods to control the Family-Wise Error Rate (FWER):
    - MCs within a single linear model (e.g. Tukey, etc.; see 08-2)
    - Bonferroni correction
      - Not recommended - overly conservative
    - Sequential Bonferroni procedure
    - Randomization procedures to empirically control FWER 
6. Methods to control the False Discovery Rate (FDR)
    - False Discovery Rate Methods
    - _Positive_ False Discovery Rate Methods
    

## False discovery rate

Proposed by Benjamini and Hochberg [-@Benjamini1995-cw].

- Also see Curran-Everett (2000).

Controls FDR (i.e., rate of Type I errors), rather than FWER

$$\mbox{FDR} = \frac{\mbox{n False Positives}}{\mbox{n All Positives}}$$

e.g., I'm OK with 5% false positives *among the tests I judge as significant*.

Note: False Positive Rate = $\frac{\mbox{n False Positives}}{\mbox{n All Tests}}$

## FDR & FPR

You do 100 correlations. You find 22 are statistically significant at an $\alpha$ = 0.05. Let's say 5 are false positives.

1. What is the FPR?
2. What is the FDR?

$$\mbox{FPR} = \frac{\mbox{n False Positives}}{\mbox{n All Tests}}$$
$$\mbox{FDR} = \frac{\mbox{n False Positives}}{\mbox{n All Positives}}$$


## FDR & FPR

$$FPR = \frac{5}{100} = 0.05$$

$$FDR = \frac{5}{22} = 0.227$$

## References
