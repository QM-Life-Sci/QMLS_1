---
title: "Unit 6 In Class Discussion"
author:
  - Elizabeth King
  - Kevin Middleton
format:
  revealjs:
    theme: [default, custom.scss]
    standalone: true
    self-contained: true
    logo: QMLS_Logo.png
    slide-number: true
    show-slide-number: all
code-annotations: hover
bibliography: QMLS_Bibliography.bib
csl: evolution.csl
---

```{r}
#| label: setup
#| message: false
#| warning: false

library(tidyverse)
library(cowplot)
library(performance)
library(see)

theme_set(theme_cowplot(font_size = 20))
```


## Quiz questions

- 6-3
- 6-4


## Assumptions of OLS

-  At each $X$, there is a normally distributed population of $Y$ observations with a mean at the regression line
-  The variance of all $Y$ observations is equal across the range of $X$. 

![](https://i.imgur.com/F0Y5Fva.png){fig-align="center" width=50%}


## Quiz 06-2

> I'm still somewhat confused about how to calculate the values of Θ~0~ and Θ~1~ for bivariate regression and the impact that adding or removing data points might have on those Θ values. I understand that Θ~0~ represents the intercept and Θ~1~ the slope, but I'm a little unclear how these values would be calculated for a particular bivariate distribution if we've already graphed the bivariate distribution in R.


## Quiz 06-3


## Diagnostics {.smaller}

> Are there any common drawbacks of bivariate regression to keep in mind when working with real-world biological data?

> I'd really like if we could go over how unequal variance changes probability distributions and how probability is calculated (if it does at all - I don't really understand how they interact mathematically). 

> When we talk about normality of residuals is that they distribute normally? And about the variance of the residuals  is it that the spread of probabilities for an observed value in y is similar for all data points? If it is that, does it mean that they are comparable or what are the implications? Is it valid to say that if a model fits part of the assumptions (let's say in the first half of the data), the model is predictable until that point?




## Quiz 06-4


## Problematic residuals

![](https://openintro-ims.netlify.app/inf-model-slr_files/figure-html/fig-whatCanGoWrongWithLinearModel-1.png){fig-align="center" width=100%}

From [Introduction to Modern Statistics](https://openintro-ims.netlify.app/) (Çetinkaya-Rundel and Hardin, 2021)


## Categorical variables

> I am interested in learning more about interpreting the model diagnostics for categorical variables. It looked like some of these tests  show two vertical lines at the 0 and 1 values. Is this what we would expect for all categorical variables in a bivariate model? Do we ignore that specific diagnostic test? What tests are important to focus on for categorical variables?

> I'm wondering if you can use performance to check normality/homogeneity for models with categorical predictors?


## Extreme Values {.smaller}

> Could you be more specific about what criteria you look for when deciding if outliers are considered influential data? I know it can vary based on the type of data and understanding the biological processes at work, however I was curious what your labs do to make those decisions. 

> I'd really like to go over plotting residuals with really small data sets and what that looks like in terms of identifying influential data. My preliminary data set has control n = 40 and experimental n = 17; the experimental group has one majorly influential outlier, but it influences the difference in groups to be drastically more significant, not less. I'm struggling with deciding if I should exclude it, keep it, or wait to decide until I finish collecting more data on the experimental group (and what to do with it in the meantime). 


## Extreme Values 

> I'd like to hear more on the decision-making process of when and when not to include outliers. Are there instances where we might want to exclude specific outliers, rather than all of them?

> I would like to know how to make decisions on when to include "outliers", for example, if I am sure they are not a mistake but they are probably influencing my results. 


## Extreme Values 

> Are there any conditions when we can include the outliers?

> How do we decide if the outliers are distorting the model and they need to be excluded? Will it not affect the variability of the data and the overall result?



## Approaches to Extreme Values

- Figure out what is going on
- Detecting (be wary of arbitrary decision thresholds)
    - Leverage
    - Cook's Distance
- Determine the impact on parameter estimates
    - Leave out and compare


## Alternate statistical approaches

- Transformation
- Randomization
- Robust regression
- Bayesian inference

We do not recommend non-parametric statistics.


## What is "linear" in linear models?

> Can you all expand more on why 'linear' does not always mean straight-line?

> I am wondering about when linearity does not necessarily mean straight-line. How would the diagnostics be interpreted in such cases?


## Can this be a linear model?

```{r}
#| echo: true
#| fig-align: center

set.seed(347347)

ED <- tibble(x = runif(50, -10, 10),
             y = 2 * x^2 + 3 * x + 4 + rnorm(50, 0, 10))
ggplot(ED, aes(x, y)) +
  geom_point(size = 3)
```


## Can this be a linear model?

```{r}
#| echo: true
#| fig-align: center

set.seed(347347)

ED <- tibble(x = runif(50, -10, 10),
             x_sq = x^2,
             y = 2 * x_sq + 3 * x + 4 + rnorm(50, 0, 10))
ggplot(ED, aes(x, y)) +
  geom_point(size = 3)
```


## Polynomial regression

```{r}
#| echo: true
#| output-location: slide

fm <- lm(y ~ x_sq + x + 1, data = ED)
summary(fm)
```


## Model check

```{r}
#| echo: true
#| output-location: slide

check_model(fm)
```


## What does "linear" mean?

- Linear combinations (`+`) of predictors:
    - `y ~ x1 + x2`
- Multiplicative effects create new variables (coming soon)
    - `y ~ x1 + x2 + x1*x2`
- Exponents, square roots, logs, etc. are transformations
    - `y ~ sqrt(x1) + x2^2 + log(x3)`

