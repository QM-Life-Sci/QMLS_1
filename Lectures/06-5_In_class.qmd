---
title: "Unit 6 In Class Discussion"
author:
  - Elizabeth King
  - Kevin Middleton
format:
  revealjs:
    theme: [default, custom.scss]
    standalone: true
    self-contained: true
    logo: QMLS_Logo.png
    slide-number: true
    show-slide-number: all
code-annotations: hover
bibliography: QMLS_Bibliography.bib
csl: evolution.csl
---

```{r}
#| label: setup
#| message: false
#| warning: false

library(tidyverse)
library(cowplot)
library(performance)
library(see)

theme_set(theme_cowplot(font_size = 20))
```


## Quiz questions

- 6-2
- 6-3
- 6-4


## Assumptions of OLS

-  At each $X$, there is a normally distributed population of $Y$ observations with a mean at the regression line

-  The variance of all $Y$ observations is equal. 

![](https://i.imgur.com/F0Y5Fva.png){fig-align="center" width="70%}


## Problematic residuals

![](https://openintro-ims.netlify.app/24-inf-model-slr_files/figure-html/whatCanGoWrongWithLinearModel-1.png){fig-align="center" width=100%}

From [Introduction to Modern Statistics](https://openintro-ims.netlify.app/) (Ã‡etinkaya-Rundel and Hardin, 2021)


## What does "Linear" models mean

> 



## Polynomials and logs are just transformations

```{r}
#| echo: true

set.seed(347347)

ED <- tibble(x = runif(50, -10, 10),
             y = 2 * x^2 + 3 * x + 4 + rnorm(50, 0, 10))
ggplot(ED, aes(x, y)) +
  geom_point()
```


## Polynomials and logs are just transformations

```{r}
#| echo: true

set.seed(347347)

ED <- tibble(x = runif(50, -10, 10),
             x_sq = x^2,
             y = 2 * x_sq + 3 * x + 4 + rnorm(50, 0, 10))
ggplot(ED, aes(x, y)) +
  geom_point()
```


## Polynomial

```{r}
#| echo: true
#| output-location: slide

fm <- lm(y ~ x_sq + x + 1, data = ED)
check_model(fm)
```



## Extreme values {.smaller}

> 

## Diagnostics

> Do you recommend running all of the different bivariate model diagnostics when we are testing our models, or are there certain situations when we should be running one independent of another?

> Does your data have to fit all the regression diagnostics tests to be reliable for linear regression?  Even in the example presented in the lecture, for some it looks okay and for some it is out of the set standard. 


## `model_check()`

```{r}
#| fig-align: center

library(performance)
fm <- lm(Petal.Width ~ Petal.Length, data = iris)
PP <- plot(check_model(fm, panel = FALSE))
PP[[2]]
```


## The data

```{r}
#| fig-align: center

library(tidyverse)
ggplot(iris, aes(Petal.Length, Petal.Width)) +
  geom_point(size = 3) +
  geom_smooth(formula = y ~ x, method = "lm", se = FALSE,
              linewidth = 1.5) +
  labs(x = "Petal Length", y = "Petal Width")
```


## The data

```{r}
#| fig-align: center

ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) +
  geom_point(size = 3) +
  geom_smooth(aes(group = 1),
              formula = y ~ x, method = "lm", se = FALSE,
              linewidth = 1.5) +
  theme(legend.position = "none") +
  labs(x = "Petal Length", y = "Petal Width")
```

