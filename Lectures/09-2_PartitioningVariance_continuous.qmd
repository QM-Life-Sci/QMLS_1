---
title: "Partitioning Variance"
subtitle: "Continuous Variables"
author:
  - Elizabeth King
  - Kevin Middleton
format:
  revealjs:
    theme: [default, custom.scss]
    standalone: true
    self-contained: true
    logo: QMLS_Logo.png
    slide-number: true
    show-slide-number: all
code-annotations: hover
bibliography: QMLS_Bibliography.bib
csl: evolution.csl
---

## Linear Models and Variance 

```{r}
#| label: setup
#| echo: false
#| message: false
#| warning: false

library(tidyverse)
library(cowplot)
theme_set(theme_cowplot())

library(readxl)
library(ggrepel)
library(plotly)
```

<!--
Datasets
  Milk.xlsx
-->


> How much of the variation in the outcome variable is explained by the predictor variables?

:::: {.columns}

::: {.column width="50%"}

![](../images/TF_variance_fig.png){fig-align="center" width="65%"}


:::

::: {.column width="50%"}

![](https://i.imgur.com/6nLlp1w.png){fig-align="center" width="60%"}

:::

::::

:::{.right}
Adapted from Tabachnick and Fidell [-@Tabachnick2018-tl]
:::


## Multiple Regression Makes a Composite Variable

- Linear combination of predictor variables that is maximally correlated with outcome variable
- How well can you predict the outcome by the set of predictor variables?
    - correlation of $y$ with $\hat{y}$ 
    - $R^{2}$ = squared correlation coefficient of $y$ with $\hat{y}$ 

    
## Milk Energy

```{r}
#| echo: true
Milk <- read_excel("../data/Milk.xlsx", na = "NA") |> 
  select(Species, Milk_Energy, Fat, Lactose) |>
  drop_na()

Milk
```


## Visualizing data

```{r fig.height=6}
fig <- plot_ly() |>
  add_markers(data = Milk,
              y = ~ Lactose,
              x = ~ Fat,
              z = ~ Milk_Energy, 
              marker = list(size = 5),
              showlegend = FALSE) |>
  hide_colorbar() |>
  layout(scene = list(xaxis = list(title = 'Lactose'),
                      yaxis = list(title = 'Fat'),
                      zaxis = list(title = 'Milk Energy')))

fig
```


## Linear model

```{r}
fm_Multi <- lm(Milk_Energy ~ Fat + Lactose, data = Milk)
summary(fm_Multi)
```


## Visualizing multiple regression

```{r fig.height=6}
#| warning: false

# https://stackoverflow.com/questions/38331198/add-regression-plane-to-3d-scatter-plot-in-plotly

# Graph resolution
graph_reso <- 0.5

# Setup axis
axis_x <- seq(min(Milk$Fat), max(Milk$Fat), by = graph_reso)
axis_y <- seq(min(Milk$Lactose), max(Milk$Lactose), by = graph_reso)

# Sample points
fm_surface <- expand.grid(Fat = axis_x,
                          Lactose = axis_y,
                          KEEP.OUT.ATTRS = FALSE)
fm_surface$Milk_Energy <- predict.lm(fm_Multi, newdata = fm_surface)
fm_surface <- reshape2::acast(fm_surface,
                              Lactose ~ Fat,
                              value.var = "Milk_Energy")

fig2 <- add_trace(p = fig,
                  z = fm_surface,
                  x = axis_x,
                  y = axis_y,
                  opacity = 0.25,
                  colorscale = list(c(0, 1), c("black", "black")),
                  type = "surface",
                  showlegend = FALSE) |>
  hide_colorbar()

fig2
```


## Multiple Regression Makes a Composite Variable

$$\hat{Y} = \theta_0 + \theta_1 Fat + \theta_2 Lactose$$

```{r}
#| echo: true

Milk |> slice(1)

coef(fm_Multi)

CC <- coef(fm_Multi)

cat(CC[1] + CC[2]*Milk$Fat[1] + CC[3]*Milk$Lactose[1])

y_hat <- fm_Multi$fitted.values
```


## Calculating $R^{2}$

```{r}

ggplot(Milk, aes(x = y_hat, y = Milk_Energy)) +
  geom_abline(slope = 1, intercept = 0, color = "blue") +
  geom_point(size = 3) +
  geom_point(data = Milk[1,], aes(x = y_hat[1], y = Milk_Energy),
             color = "coral", size = 3) +
  labs(y = 'Milk Energy', x = 'Predicted Milk Energy') +
  xlim(c(0.3, 1)) +
  ylim(c(0.3, 1)) +
  coord_equal()
```


## Calculating $R^{2}$

Squared correlation of the observed and predicted values:

```{r}
#| echo: true

cor(y_hat, Milk$Milk_Energy)^2
```

From the model summary:

```{r}
#| echo: true
summary(fm_Multi)$r.squared
```

## $R^{2}$


$$Multiple~R^{2} = \frac{(a + b + c)}{(a + b + c + d)}$$

![](../images/TF_variance_fig.png){fig-align="center" width="80%"}


## Variance explained by individual predictors

:::: {.columns}

::: {.column width="50%"}

- Only the portion of variance in $y$ independently explained by each predictor
- For x1:

$$\frac{a}{(a + b + c + d)}$$
    
:::

::: {.column width="50%"}

![](../images/TF_variance_fig.png){fig-align="center" width="60%"}

:::

::::

## Variance explained by individual predictors

```{r}
#| echo: true

fm_Multi <- lm(Milk_Energy ~ Fat + Lactose, data = Milk)
sum_Multi <- summary(fm_Multi)

fm_Fat <- lm(Milk_Energy ~ Fat, data = Milk)
sum_Fat <- summary(fm_Fat)

fm_Lactose <- lm(Milk_Energy ~ Lactose, data = Milk)
sum_Lactose <- summary(fm_Lactose)

cat(sum_Multi$r.squared, "\t", sum_Fat$r.squared,"\t", sum_Lactose$r.squared)

sum_Multi$r.squared - sum_Fat$r.squared
sum_Multi$r.squared - sum_Lactose$r.squared

```

## Colinearity = High Shared Variation

```{r}

library(GGally)
ggscatmat(Milk, columns = 2:4)

```

## Partitioning variation with uncorrelated data

:::: {.columns}

::: {.column width="50%"}

```{r, fig.height = 6, fig.width = 6}
set.seed(189324)

nn <- 100

v1 <- rnorm(nn)
v2 <- rnorm(nn)

y <- 1.2*v1 + 2*v2 + rnorm(nn, 0,1)

dd <- tibble(y = y, v1 = v1, v2 = v2)

ggscatmat(dd, columns = 1:3)

```
    
:::

::: {.column width="50%"}

![](../images/TF_variance_fig_nocorr.png){fig-align="center" width="60%"}

:::

::::

## Variance explained by individual predictors

```{r}
#| echo: true

fm_Multi <- lm(y ~ v1 + v2, data = dd)
sum_Multi <- summary(fm_Multi)

fm_v1 <- lm(y ~ v1, data = dd)
sum_v1 <- summary(fm_v1)

fm_v2 <- lm(y ~ v2, data = dd)
sum_v2 <- summary(fm_v2)

cat(sum_Multi$r.squared, "\t", sum_v1$r.squared, "\t", sum_v2$r.squared)

sum_Multi$r.squared - sum_v1$r.squared
sum_Multi$r.squared - sum_v2$r.squared

```


## References

::: {#refs}
:::

