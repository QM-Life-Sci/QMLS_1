---
title: "Progress Check 3 Review Session"
author:
  - Elizabeth King
  - Kevin Middleton
format:
  revealjs:
    theme: [default, custom.scss]
    standalone: true
    self-contained: true
    logo: QMLS_Logo.png
    slide-number: true
    show-slide-number: all
code-annotations: hover
bibliography: QMLS_Bibliography.bib
csl: evolution.csl
---

```{r}
library(tidyverse)
library(cowplot)
theme_set(theme_cowplot())

```

## Course Evaluations

- Instructor specific (so there are 2 - sorry!)
- Open until May 10th
- Already have an email
- Can't find email? 
    - Go to [evaluation.missouri.edu](https://evaluation.missouri.edu/)


## Wrapping up the semester

- Grades are due Tuesday, May 20th
- Please look to see if you have incomplete assignments and submit them
    - Including posting questions or replies
- If you are unable to complete your missing assignments, set up a time to talk to us about an incomplete

    
## Course Materials

- [Github](https://github.com/QM-Life-Sci/QMLS_1)
    - Slides
    - Code
    - Problem sets, keys
- [YouTube](https://www.youtube.com/watch?v=bMRfrufqu28&list=PLeIk-G-wZpqyys1lUuWQDxeMQA1ICncmH)


## Quant Methods 2 (BIO_SC 8642)

- Current modules (5 weeks; 1 unit each)
    - Advanced data visualization
    - Bayesian inference 1
    - Randomization and simulation methods
    - Multivariate methods


## Quant Methods 2

- Possible future modules
    - Bayesian inference 2
    - Multilevel models (e.g., GWAS, spatial, kinship, phylogenetic)
    - Reproducible Research Methods
    - Advanced R Programming


## Problem set 12

- Could we go over interpreting the results of the analysis for the Heart Transplant Survivorship section in PS 12?
- I had a question about the parallelization randomization in PS12. What does the function 'future_map_dbl' do? My group for this problem all structured the looping differently and we got slightly different p-values. I would like to walk through this problem again to be sure I am understanding how to set up the structure and flow of randomization. 
- In PS 12 I didn't understand the point of fitting a linear model with the PC1, and how to interpret those results.


## Linear Models

- If interactions (variable1*variable2) have a low power to be detected, should we aim in those cases to have a huge increase in sample size?
- I think what would help me a lot, is if we went over in summary the different types of linear models and what the values (coefficients, intercept, statistics, etc.) mean practically. At this point, even though I understand much better than at the beginning, I still feel like I default to looking for something significant without thinking too deeply about what the data is saying.


## lm / lme / etc.

- What are the differences between lm() and glm()? When can you use lm() and when can you use glm()?  how do you interpret when you get the same output from both functions? 
- While selecting a model, how would you choose between lm() and lme() for a new dataset with grouped structure and what criteria (AIC, LRT) could help decide whether to include a random effect?


## What to use when

:::: {.columns}

::: {.column width="40%"}
`lm()`: Linear models

- OLS regression
- *t*-tests
- ANOVA
- Multiple regression 
:::

::: {.column width="60%"}
`lme()`, `lme4()`, `glmmTMB()`

- Multilevel linear models 

---

`glm()`: Generalized linear models

- Logistic
- Poisson
- Others

:::

::::


## Randomization and Non-parametric tests

- Still some studies use traditional non-parametric tests, do you consider that we should never use them again and stick to modern methods like randomization? Are they still useful for some context?


## Model Comparison

- I'd be interested in reviewing how to determine the predictive strength of a particular linear model through likelihood ratio, AICc, and cross-validation, particularly when comparing the results of each of these tests on a particular model to one another, and what information each of these tests can provide about the linear model itself in addition to an indication of predictive strength. 


## Model comparison: When to use what method

1. Likelihood ratio tests
2. AIC(c) / Model weights
3. Cross-validation

All will generally agree.


## Model comparison: Possible goals

::: {.incremental}

- Explicit P-value comparisons between models (LRT)
- Out-of-sample predictive ability: qualitative (AICc)
- Relative model support (AICc -> model weights)
- Out-of-sample predictive ability: quantitative (cross-validation)

:::


## PCA

- In some studies, PCA is used to discriminate or differentiate groups. Is it a correct approach as they don't use anything else like fitting a model or any posterior analysis to validate.
- My question stems from a conversation I saw somewhere where some people argued that PCA should only be used for modelling linear relationships and not in non-linear relationships cases. Essentially, does PCA suffice for most data as per linear vs non-linear? 


## Progress Check 3

- For the guppy problem, we are asked to "perform this simulation (lm and lme models) 5,000 times, storing the *P*-values from each iteration into either separate vectors or a `tibble`/`data.frame` with one column for each analysis (lm and lme)." Could we review how to return just the P values from the general lm?
- Re: making `Diff_Elev` predicted by just a grand mean (Intercept term) Wouldn't the model with Diff_Elev predicted by treatment already have this intercept term? Does it need to be its own model?


## Tips

Could we please go over useful tips you would recommend we do to stay caught up with relevant functions within R?
