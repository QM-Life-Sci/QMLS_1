---
title: "Bivariate Regression"
author:
  - Elizabeth King
  - Kevin Middleton
format:
  revealjs:
    theme: [default, custom.scss]
    standalone: true
    self-contained: true
    logo: QMLS_Logo.png
    slide-number: true
    show-slide-number: all
code-annotations: hover
bibliography: QMLS_Bibliography.bib
csl: evolution.csl
---


## For example...

1. Calibration curves
2. Metabolic rate vs. Body mass
3. Leaf area vs. Total rainfall

Continuous variable modeled by one continuous variable


```{r}
#| label: setup
#| message: false
#| warning: false

library(tidyverse)
library(cowplot)
library(latex2exp)
library(cmdstanr)
library(bayesplot)
library(posterior)

theme_set(theme_cowplot())

set.seed(8371374)

source("QMLS_functions.R")
```


## Synonyms

- Bivariate regression
- Linear regression
- Ordinary least squares (OLS) regression

All refer to a linear model where a normally distributed outcome variable is modeled by a single continuous predictor


## Goal

How do we model the relationship between leaf area and rainfall?

```{r}
#| eval: true

dd <- tibble("rain" = rnorm(50, 800, 100))
dd$LAI <- dd$rain/400 + rnorm(50,0,0.5)

ggplot(dd, aes(rain, LAI)) +
  geom_point(size = 3, color = "navy") +
  xlab("Rainfall (mm)") +
  ylab("Leaf Area Index")

```


## Goal

What values of $\theta_0$ (intercept) and $\theta_1$ (slope) provide the best fit line through $Y$ as a function of $X$?

$$Y = \theta_0 + \theta_1 X$$

![](https://i.imgur.com/hbkIVLg.gif){fig-align="center" width="30%}

- How do we estimate $\theta_0$ and $\theta_1$?
- What is "best fit"?


## Generate data

Generate $n = 30$ random data points: 

- $X \sim Normal(mean = 10, sd = 1)$
- $Y = 2.3 X + \epsilon$ ("functional relationship")
- where $\epsilon \sim Normal(1, 1)$ ("noise")

```{r}
#| echo: true
#| output-location: slide

set.seed(4)
n <- 30
X <- rnorm(n, mean = 10, sd = 1)
Y <- 2.3 * X + rnorm(n, mean = 1, sd = 1)
M <- data.frame(X, Y)
M
```


## Generate data

```{r}
p <- ggplot(M, aes(X, Y)) + 
  geom_point(size = 3, color = "navy")
p
```


## Model characteristics

An infinite set of possible slopes ($\theta_1$) and intercepts ($\theta_0$)

1. Slopes and intercepts are tied together
1. All lines must pass through $\left(\bar{X}, \bar{Y}\right)$.
1. Sum of the squared deviations vary continuously.
1. Only one value of $\theta_1$ will minimize the SS (and maximize the likelihood).
    - The *Ordinary Least Squares* estimate


## Assumptions of OLS

-  At each $X$, there is a normally distributed population of $Y$ observations with a mean at the regression line

-  The variance of all $Y$ observations is equal. 

![](https://i.imgur.com/F0Y5Fva.png){fig-align="center" width="70%}


## Assumptions of OLS 

:::: {.columns}

::: {.column width="50%"}

Few assumptions are made about $X$

- *Is* measured without error
- *Not* that it is normal or that it is randomly sampled
- Think about calibration curves. The $X$ observations are fixed.

:::

::: {.column width="50%"}

```{r, fig.height = 5, fig.width = 5}

#| echo: false
#| eval: true
set.seed(86)
cc <- tibble("Concentration" = seq(from = 10, to = 110, by = 10))
cc$Absorbance = cc$Concentration * 0.025 + rnorm(nrow(cc), 0, 0.15)

ggplot(cc, aes(Concentration, Absorbance)) +
  geom_point(size = 3) +
  scale_x_continuous(n.breaks = 10) +
  theme(text = element_text(size = 20))
  
```

:::

::::


## Fitting a Model

How would probabilities change for different slope estimates?

:::: {.columns}

::: {.column width="50%"}

![](https://i.imgur.com/F0Y5Fva.png){fig-align="center"}

:::

::: {.column width="50%"}

```{r}
#| echo: true
#| eval: false

Y <- 2.3 * X + 
  rnorm(n, mean = 1, sd = 1)

```

```{r}
#| fig-height: 7

p + 
  geom_abline(slope = 2.3, intercept = 0, color = "blue", linewidth = 1) +
  geom_abline(slope = 1.4, intercept = 10, color = "grey50", linewidth = 1) +
  geom_abline(slope = 2.0, intercept = 2, color = "grey50", linewidth = 1) +
  geom_abline(slope = 3.5, intercept = -10, color = "grey50", linewidth = 1) 
  
```

:::

::::


## What is the likelihood of an observed value given a regression model? 

Define a function to calculate the likelihood of an observed value $Y_i$ given the mean ($\mu$) and standard deviation ($\sigma$). Default to the standard normal distribution: $Normal(0,1)$.

$$Pr\left[Y_i\right] = \frac{1}{\sqrt{2\pi\sigma^{2}}} e^{\frac{-\left(Y_i-\mu\right)^{2}}{2\sigma^{2}}}$$


## Define a model

$$y = \bar{Y} + \theta_1 X$$

$$\theta_1 = 0$$

This is a horizontal line ($\theta_1 = 0$) through the mean of $Y$.


## Define a model

```{r}
#| results: hide

set.seed(4)
n <- 30
X <- rnorm(n, mean = 10, sd = 1)
Y <- 2.3 * X + rnorm(n, mean = 1, sd = 1)
M <- data.frame(X, Y)

ssPlot(X, Y, b = 0, do.labels = FALSE)
```


## Calculate the predicted values

Just the mean of $Y$ repeated 30 times.

```{r}
#| echo: true

Y_bar <- mean(Y)
Y_hat <- rep(Y_bar, length(Y))
Y_hat
```


## Likelihood of a predicted value of $Y$

$$Pr\left[Y_i\right]=\frac{1}{\sqrt{2\pi\hat{\sigma}^2}} e^{\frac{-\left(Y_i - \mu\right)^{2}}{2\hat{\sigma}^2}}$$

- $\mu =$ the predicted value $\hat{Y}_i$
- Standard deviation ($\hat{\sigma}$) can be calculated based on the variance of $Y$.

```{r}
#| echo: false

# Estimated variance
var_hat <- sum((Y - Y_hat)^2) / (length(Y))
sd_hat <- sqrt(var_hat)

```


## Likelihoods for observed $Y$s

Check the likelihood for the first $Y$:

```{r}
#| echo: true
dnorm(Y[1], mean = Y_hat[1], sd = sd_hat)
```

Calculate for all $Y$s

```{r}
#| echo: true
(liks_Y <- dnorm(Y, mean = Y_hat, sd = sd_hat))
```


## Model Likelihood ($\mathcal{L}$)

For a set of $Y_i$ and parameters ($\Theta$; i.e., slope and intercept) the likelihood of the model is the product of their individual probabilities:

$$\mathcal{L}\left(\left\{ Y_{i}\right\} _{i=1}^{n};\Theta\right) = \prod_{i=1}^{n}Pr\left[Y_{i}; \Theta\right]$$

$$\log\left(\mathcal{L}\left(\left\{ Y_{i}\right\} _{i=1}^{n};\Theta\right)\right) = \sum_{i=1}^{n} \log\left(Pr\left[Y_{i};\Theta\right]\right)$$


## Model Likelihood ($\mathcal{L}$)

Sum of the log-likelihoods:

```{r}
#| echo: true

sum(log(liks_Y))
```


## Maximizing the log-Likelihood

Function to calculate the log-likelihood. Input the slope ($\theta_1$) and observed values of $X$ and $Y$. Return the log-likelihood.

```{r log_lik}
#| echo: true

log_lik <- function(theta_1, X, Y){
  Y_bar <- mean(Y)
  X_bar <- mean(X)
  theta_0 <- Y_bar - theta_1 * X_bar
  Y_hat <- theta_0 + theta_1 * X
  var_hat <- sum((Y - Y_hat) ^ 2) / (length(Y))
  sd_hat <- sqrt(var_hat)
  liks_Y <- dnorm(Y, mean = Y_hat, sd = sd_hat, log = TRUE)
  return(sum(liks_Y))
}

log_lik(2.3, X, Y)
```


## Minimizing Sums of Squares

```{r}
p
```


## Minimizing Sums of Squares

```{r echo=FALSE}
y_bar <- paste("bar(Y)==", round(mean(Y), 2))
x_bar <- paste("bar(X)==", round(mean(X), 2))

p +
  geom_point(data = tibble(X = mean(X), Y = mean(Y)),
             aes(X, Y), color = "navy", size = 7, pch = 1) +
  annotate("text", x = 9.25, y= 28 ,
           label = y_bar,
           parse = TRUE,
           size = 7) +
  annotate("text", x = 9.25, y= 27 ,
           label = x_bar,
           parse = TRUE,
           size = 7)
```


## Deviates

```{r}
# b = 0
ss <- ssPlot(X, Y, 0, do.labels = FALSE)
```


## $\theta_1 = 0$

```{r}
# b = 0
ss <- ssPlot(X, Y, 0)
```


## $\theta_1 = 0.5$

```{r}
ss <- ssPlot(X, Y, 0.5)
```


## $\theta_1 = 1$

```{r}
ss <- ssPlot(X, Y, 1)
```


## $\theta_1 = 1.5$

```{r}
ss <- ssPlot(X, Y, 1.5)
```


## $\theta_1 = 2$

```{r}
ss <- ssPlot(X, Y, 2)
```


## $\theta_1 = 2.1$

```{r}
ss <- ssPlot(X, Y, 2.1)
```


## Minimizing Sums of Squares

- Search a range of values for $\theta_1$

```{r}
#| echo: true

# To hold output
SumSq <- tibble(theta_1 = seq(-10, 10, by = 0.01),
                SS = numeric(length(theta_1)))
head(SumSq)
```


## Minimizing Sums of Squares

```{r}
#| echo: true

# Iterate through slopes
for (ii in 1:nrow(SumSq)) {
  theta_1 <- SumSq$theta_1[ii]
  SumSq$SS[ii] <- ssPlot(X, Y, theta_1, do.plot = FALSE)
}

# Location of minimum SS
minSS.theta_1 <- SumSq$theta_1[which.min(SumSq$SS)]
minSS.SS <- SumSq$SS[which.min(SumSq$SS)]
```


## Minimizing Sums of Squares

```{r}
xpos <- min(SumSq$theta_1) + 0.55 * diff(range(SumSq$theta_1))
ypos <- min(SumSq$SS) + 0.3 * diff(range(SumSq$SS))

v1 <- round(minSS.theta_1, 2)
v2 <- round(minSS.SS, 2)
ll1 <- glue::glue("$\\theta_1 = {v1}$")
ll2 <- glue::glue("$\\SS = {v2}$")

ggplot() +
  geom_line(data = SumSq, aes(x = theta_1, y = SS), linewidth = 1) +
  geom_point(data = tibble(x = minSS.theta_1, y = minSS.SS),
             aes(x, y), color = "red", size = 4) +
  labs(x = TeX("$\\theta_1$"),
       y = "Sum of Squares") +
  annotate(geom = "text",
           label = TeX(ll1, output = "character"),
           x = xpos, y = ypos,
           parse = TRUE,
           hjust = 0,
           size = 9) +
  annotate(geom = "text",
           label = TeX(ll2, output = "character"),
           x = xpos, y = ypos * 0.70,
           parse = TRUE,
           hjust = 0,
           size = 9)
```


## Closed form calculation of $\theta_1$ and $\theta_0$

$$\theta_1 = \frac{\sum\left(X_{i}-\bar{X}\right)\left(Y_{i}-\bar{Y}\right)}{\sum\left(X_{i}-\bar{X}\right)^{2}}$$

Numerator:  Sum of the products of *X* and *Y*

Denominator: Sum of squares of *X*

$$\theta_0 = \bar{Y} - \theta_1 \bar{X}$$


## Bivariate regression as a linear model

- We don't have to manually search for values of intercept ($\theta_0$) and slope ($\theta_1$) or use an optimizaiton function (what maximum likelihood would do)
- We want to solve the closed form solution directly
    - Matrix algebra

R has a function to do this.


## Introduction to `lm()`

```{r}
#| echo: true

my_model <- lm(Y ~ X, data = M)

my_model

```


## Model formulas

Y ~ X

- Response variable (Y)
- Predictor variable (X)
- Equivalent to: $$Y = \theta_0 + \theta_1 X$$


## Summary from `lm()`

```{r}
#| echo: true

summary(my_model)

```


## Likelihood from linear regression 

Fit a linear model (`lm()`) with only an intercept (`~ 1`) and use the built-in function `logLik()` to extract the log-likelihood.

```{r}
#| echo: true

fm <- lm(Y ~ 1)
logLik(fm)
```

Using our manual `log_lik()` function:

```{r}
#| echo: true
log_lik(0, X, Y)
```


## Likelihood from linear regression 

Minimizing the sum of squares *equals* maximizing the likelihood

```{r}
#| echo: true

logLik(my_model)

log_lik(2.39525, X, Y)
```

```{r}
#| fig-height: 4
#| fig-align: center

ggplot() +
  geom_line(data = SumSq, aes(x = theta_1, y = SS), linewidth = 1) +
  geom_point(data = tibble(x = minSS.theta_1, y = minSS.SS),
             aes(x, y), color = "red", size = 4) +
  labs(x = TeX("$\\theta_1$"),
       y = "Sum of Squares")

```


## Bayesian framework

- Likelihood frameworks allows any values of $\theta_0$ and $\theta_1$
- What if we have *a priori* information? (e.g., $\theta_1$ should be positive)
    - Leaf area should increase with rainfall
    - Absorbance should increase with concentration
- Bayesian framework uses priors to inform the model


## Normal(0, 10) priors for intercept ($\theta_0$) and slope ($\theta_1$)?

```{r}
#| fig-width: 14
#| fig-align: center

p1 <- M |> 
  ggplot(aes(X, Y)) +
  geom_point()
x <- seq(-40, 40, length = 100)
p2 <- tibble(x = x, y = dnorm(x, 0, 10)) |> 
  ggplot(aes(x, y)) +
  geom_line() +
  labs(x = "Theta", y = "Probability")
plot_grid(p1, p2, ncol = 2)
```

~95% of the probability is between -20 and 20


## Bayes model

```{r}
#| echo: true

model <- "
  data{
    int N;
    vector[N] Y;
    vector[N] X;
  }
  parameters{
    real theta_0;
    real theta_1;
    real<lower=0> sigma;
  }
  model{
    // Priors
    theta_0 ~ normal(0, 10);
    theta_1 ~ normal(0, 10);
    sigma ~ normal(0, 1);
    
    // Model
    Y ~ normal(theta_0 + theta_1 * X, sigma);
  }
"
```


## Sample the model...

```{r}
#| label: stan_fit
#| message: false
#| warning: false
#| results: hide

if (FALSE) {
  # Only run this interactively to avoid needing to refit the model
  # each time you render the slides. Posterior samples are saved out
  # to a qs file to save space.
  bayes_model <- cmdstan_model(stan_file = write_stan_file(model))
  fm_bayes <- bayes_model$sample(
    data = list(N = nrow(M),
                Y = M$Y,
                X = M$X),
    iter_warmup = 2500,
    iter_sampling = 2500,
    seed = 2236726,
    chains = 4)
  samples <- fm_bayes$draws(variables = c("theta_0", "theta_1", "sigma"))
  write_rds(x = samples, file = "../data/bayes_model_OLS.rds")
}

# Read the posterior samples from the saved file.
post <- read_rds("../data/bayes_model_OLS.rds")

mcmc_trace(post)
```


## Summarizing the results

1. `theta_0` = Posterior estimates for intercept
1. `theta_1` = Posterior estimates for slope
1. `sigma` = Standard deviation of the mean

```{r}
#| fig-width: 13
#| fig-align: center

mcmc_dens(post)
```


## Comparison of coefficients

Median values of posteriors:

```{r}
as_draws_df(post) |> 
  as.data.frame() |> 
  select(starts_with("theta")) |> 
  summarise(across(everything(), median)) |> 
  print(digits = 4)
```

`lm()` coefficients:

```{r}
coef(lm(Y ~ X, data = M))
```


## Bayesian posterior regression lines

```{r}
post_samp <- as_draws_df(post) |>
  as_tibble() |> 
  sample_n(200)
ggplot() +
  geom_abline(data = post_samp, aes(intercept = theta_0, slope = theta_1),
              alpha = 0.2, color = "firebrick") +
  geom_point(data = M, aes(X, Y), size = 3, color = "navy")
```

